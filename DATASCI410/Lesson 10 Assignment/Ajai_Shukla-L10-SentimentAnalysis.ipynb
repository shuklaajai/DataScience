{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10 - Sentiment Analysis\n",
    "Author: Ajai Shukla\n",
    "This assignment requires that you build a sentiment analysis classifier for a series of tweets.\n",
    "The data consists of a file \"twitter_data.csv\". The file contains 16,000 tweets with their respective score. The attributes are the sentences, and the score is either 4 (for positive) or 0 (for negative).\n",
    "\n",
    "Assignment Instructions\n",
    "1. Complete all questions below.\n",
    "2. Comment on the applicability of the model on future tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge arrow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge editdistance -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rake-nltk (1.0.4)                - Python implementation of the Rapid\n",
      "                                   Automatic Keyword Extraction algorithm\n",
      "                                   using NLTK\n",
      "nltk (3.4.3)                     - Natural Language Toolkit\n",
      "  INSTALLED: 3.4\n",
      "  LATEST:    3.4.3\n",
      "ko-nltk (0.1)                    - \n",
      "nltkrest (0.12)                  - NLTK as a REST service\n",
      "nltk_tgrep (1.0.6)               - tgrep2 Searching for NLTK Trees\n",
      "nltkjsonnlp (0.0.5)              - The Python NLTK JSON-NLP package\n",
      "lektor-natural-language (0.3.1)  - Adds NLTK based template filters.\n",
      "cnpt-leos (0.1.1)                - Custom NLTK part of speech tagger\n",
      "bluestocking (0.1.2)             - An information extraction toolkit built on\n",
      "                                   top of NLTK.\n",
      "wordgrapher (0.3.1)              - Word Graph utility built with NLTK and\n",
      "                                   TextBlob\n",
      "text_operations (0)              - A set of functions that piggyback off nltk\n",
      "                                   to parse text.\n",
      "luvina (0.0.26)                  - High-level API for Natural Language\n",
      "                                   Processing in NLTK\n",
      "bookgen (1.0.0)                  - Generates books based on other books using\n",
      "                                   nltk\n",
      "metanl (0.5.6)                   - Multilingual natural language tools,\n",
      "                                   wrapping NLTK and other systems.\n",
      "python-pkg (0.0.2)               - Un paquete de NLTK y Scrapy por SEO\n",
      "SloPOS (1.0)                     - Part of speech tagger for Slovenian (SI)\n",
      "                                   language based on NLTK\n",
      "smart-reading (1.1.6)            - An NLTK-based toolkit aimed at increasing\n",
      "                                   the understanding of various texts.\n",
      "namextracter (0.0.1)             - A lite library for extracting names from\n",
      "                                   text using nltk wordnet\n",
      "pysummarize (0.6.0)              - Simple multi-language Python and NLTK-based\n",
      "                                   implementation of text summarization\n",
      "bedrock (0.1.0.dev10)            - Bedrock is a high-level text pre-processing\n",
      "                                   API, written in Python and can run on NLTK\n",
      "                                   or Spacy as its backends.\n",
      "namebot (0.1.5)                  - A company/product name generating tool\n",
      "                                   written in Python.Uses NLTK and diverse\n",
      "                                   wordplay techniques forsophisticated word\n",
      "                                   generation and ideation\n"
     ]
    }
   ],
   "source": [
    "!pip search nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /anaconda3:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "nltk                      3.4                      py37_1  \r\n"
     ]
    }
   ],
   "source": [
    "!conda list nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - nltk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.3.9           |           py37_0         155 KB\n",
      "    conda-4.6.14               |           py37_0         2.1 MB\n",
      "    openssl-1.1.1b             |       h1de35cc_1         3.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         5.7 MB\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                       conda-forge --> pkgs/main\n",
      "  conda                                         conda-forge --> pkgs/main\n",
      "  openssl                                       conda-forge --> pkgs/main\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import csv\n",
    "import editdistance\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Specialty libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import Counter\n",
    "import argparse\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming(101101, 100011) = 3\n",
      "Hamming(bear, beer) = 1\n"
     ]
    }
   ],
   "source": [
    "# Hamming Distance\n",
    "def hamming_dist(text1, text2):\n",
    "    # For strings of equal length:\n",
    "    assert len(text1) == len(text2)\n",
    "    return sum(char1 != char2 for char1, char2 in zip(text1, text2))\n",
    "\n",
    "print('Hamming(101101, 100011) = {}'.format(hamming_dist('101101', '100011')))\n",
    "print('Hamming(bear, beer) = {}'.format(hamming_dist('bear', 'beer')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lev_Dist(bear, beer) = 1\n",
      "Lev_Dist(banana, ban) = 3\n"
     ]
    }
   ],
   "source": [
    "# Levenshtein Distance (edit distance)\n",
    "print('Lev_Dist(bear, beer) = {}'.format(editdistance.eval('bear', 'beer')))\n",
    "\n",
    "print('Lev_Dist(banana, ban) = {}'.format(editdistance.eval('banana', 'ban')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard(bear, beer) = 0.25\n",
      "Jaccard(banana, ban) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Jaccard Index Distance\n",
    "def jaccard_ix_dist(text1, text2):\n",
    "    set1 = set(list(text1))\n",
    "    set2 = set(list(text2))\n",
    "    return 1 - (len(set1.intersection(set2)) / float(len(set1.union(set2))))\n",
    "\n",
    "\n",
    "print('Jaccard(bear, beer) = {}'.format(jaccard_ix_dist('beer', 'bear')))\n",
    "\n",
    "# The following may be an issue...\n",
    "print('Jaccard(banana, ban) = {}'.format(jaccard_ix_dist('banana', 'ban')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Jaccard(bear, beer) = 0.4\n",
      "Weighted Jaccard(banana, ban) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Weighted Jaccard Index\n",
    "def weighted_jaccard_dist(text1, text2):\n",
    "    set1 = set(list(text1))\n",
    "    set2 = set(list(text2))\n",
    "    union = set1.union(set2)\n",
    "    \n",
    "    min_sum = 0\n",
    "    max_sum = 0\n",
    "    for char in union:\n",
    "        min_sum += min(text1.count(char), text2.count(char))\n",
    "        max_sum += max(text1.count(char), text2.count(char))\n",
    "    \n",
    "    return 1 - (min_sum / max_sum)\n",
    "\n",
    "print('Weighted Jaccard(bear, beer) = {}'.format(weighted_jaccard_dist('beer', 'bear')))\n",
    "\n",
    "print('Weighted Jaccard(banana, ban) = {}'.format(weighted_jaccard_dist('banana', 'ban')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfiltered text: \n",
      "I <3 statistics $\\ \\ $, it’s my ၲ  $\\ \\ $    fAvoRitE!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning example\n",
    "horrible_tweet_text = 'I <3 statistics $\\ \\ $, it’s my \\u1072  $\\ \\ $    fAvoRitE!! 11!!!'\n",
    "print('Unfiltered text: \\n{}\\n'.format(horrible_tweet_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed non-ascii unicode: \n",
      "I <3 statistics $\\ \\ $, its my   $\\ \\ $    fAvoRitE!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove non-ascii unicode\n",
    "clean1 = ''.join([x for x in horrible_tweet_text if ord(x) < 128])\n",
    "print('Removed non-ascii unicode: \\n{}\\n'.format(clean1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased: \n",
      "i <3 statistics $\\ \\ $, its my   $\\ \\ $    favorite!! 11!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove uppercase\n",
    "clean2 = clean1.lower()\n",
    "print('Lowercased: \\n{}\\n'.format(clean2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Punctuation: \n",
      "i 3 statistics    its my         favorite 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "clean3 = ''.join(char for char in clean2 if char not in exclude)\n",
    "print('No Punctuation: \\n{}\\n'.format(clean3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed numbers: \n",
      "i  statistics    its my         favorite \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers\n",
    "clean4 = re.sub(\"\\d+\", \"\", clean3)\n",
    "print('Removed numbers: \\n{}\\n'.format(clean4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stripped extra whitespace: \n",
      "i statistics its my favorite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Strip extra whitespace\n",
    "clean5 = ' '.join(clean4.split())\n",
    "print('Stripped extra whitespace: \\n{}\\n'.format(clean5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed stopwords like: ['i', 'me', 'my', 'myself', 'we'] ...\n",
      "statistics favorite\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "word_list = clean5.split(' ')\n",
    "clean_word_list = [word for word in word_list if word not in stopwords.words('english')]\n",
    "clean6 = ' '.join(clean_word_list)\n",
    "print('Removed stopwords like: {} ...'.format(stopwords.words('english')[0:5]))\n",
    "print(clean6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Words: \n",
      "statistic favorite\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stem Words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stemmed_words = [lmtzr.lemmatize(word) for word in clean_word_list]\n",
    "clean7 = ' '.join(stemmed_words)\n",
    "print('\\nStemmed Words: \\n{}\\n'.format(clean7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic favorite\n"
     ]
    }
   ],
   "source": [
    "# Create a function to do this\n",
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_stopwords':\n",
    "            stops = stopwords.words('english')\n",
    "            word_list = text.split(' ')\n",
    "            text_words = [word for word in word_list if word not in stops]\n",
    "            text = ' '.join(text_words)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "step_list = ['remove_non_ascii', 'lowercase', 'remove_punctuation', 'remove_numbers',\n",
    "            'strip_whitespace', 'remove_stopwords', 'stem_words']\n",
    "print(preprocess(horrible_tweet_text, step_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text\n",
      "0                4  @elephantbird Hey dear, Happy Friday to You  A...\n",
      "1                4  Ughhh layin downnnn    Waiting for zeina to co...\n",
      "2                0  @greeniebach I reckon he'll play, even if he's...\n",
      "3                0              @vaLewee I know!  Saw it on the news!\n",
      "4                0  very sad that http://www.fabchannel.com/ has c...\n"
     ]
    }
   ],
   "source": [
    "#Read files\n",
    "url = \"https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/twitter_data.csv\"\n",
    "df = pd.read_csv(url, sep=\",\")\n",
    "df.columns = [\"sentiment_label\",\"tweet_text\"]\n",
    "    \n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Generate word cloud for positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text\n",
      "0                1  @elephantbird Hey dear, Happy Friday to You  A...\n",
      "1                1  Ughhh layin downnnn    Waiting for zeina to co...\n",
      "2                0  @greeniebach I reckon he'll play, even if he's...\n",
      "3                0              @vaLewee I know!  Saw it on the news!\n",
      "4                0  very sad that http://www.fabchannel.com/ has c...\n",
      "\n",
      "\n",
      "count    160000.000000\n",
      "mean          0.500000\n",
      "std           0.500002\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.500000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "Name: sentiment_label, dtype: float64\n",
      "\n",
      "\n",
      " Count of positives: 80000\n"
     ]
    }
   ],
   "source": [
    "# But sentiment is either '4' or '0'. We'll change that to '1' or '0' to indicate positive or negative sentiment.\n",
    "df.sentiment_label=df.sentiment_label.replace(4,1)\n",
    "\n",
    "# Check the Data frame again\n",
    "print(df.head())\n",
    "\n",
    "print('\\n\\n{}'.format(df['sentiment_label'].describe()))\n",
    "\n",
    "print('\\n\\n Count of positives: {}'.format(np.sum(df['sentiment_label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Generate word cloud for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text\n",
      "0                1  @elephantbird Hey dear, Happy Friday to You  A...\n",
      "1                1  Ughhh layin downnnn    Waiting for zeina to co...\n",
      "2                0  @greeniebach I reckon he'll play, even if he's...\n",
      "3                0              @vaLewee I know!  Saw it on the news!\n",
      "4                0  very sad that http://www.fabchannel.com/ has c...\n",
      "\n",
      "\n",
      "count    160000.000000\n",
      "mean          0.500000\n",
      "std           0.500002\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.500000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "Name: sentiment_label, dtype: float64\n",
      "\n",
      "\n",
      " Count of negatives: 80000\n"
     ]
    }
   ],
   "source": [
    "# But sentiment is either '4' or '0'. We'll change that to '1' or '0' to indicate positive or negative sentiment.\n",
    "df.sentiment_label=df.sentiment_label.replace(4,1)\n",
    "print(df.head())\n",
    "\n",
    "print('\\n\\n{}'.format(df['sentiment_label'].describe()))\n",
    "\n",
    "print('\\n\\n Count of negatives: {}'.format(np.sum(df['sentiment_label'])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Split data into 70% for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a copy of the tweets as list for use later\n",
    "tweet_data = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean tweets\n",
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace']\n",
    "\n",
    "df['clean_tweet'] = df['tweet_text'].map(lambda s: preprocess(s, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@elephantbird Hey dear, Happy Friday to You  A...</td>\n",
       "      <td>elephantbird hey dear happy friday to you alre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ughhh layin downnnn    Waiting for zeina to co...</td>\n",
       "      <td>ughhh layin downnnn waiting for zeina to cook ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@greeniebach I reckon he'll play, even if he's...</td>\n",
       "      <td>greeniebach i reckon hell play even if hes not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@vaLewee I know!  Saw it on the news!</td>\n",
       "      <td>valewee i know saw it on the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>very sad that http://www.fabchannel.com/ has c...</td>\n",
       "      <td>very sad that httpwwwfabchannelcom has closed ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_label                                         tweet_text  \\\n",
       "0                1  @elephantbird Hey dear, Happy Friday to You  A...   \n",
       "1                1  Ughhh layin downnnn    Waiting for zeina to co...   \n",
       "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
       "3                0              @vaLewee I know!  Saw it on the news!   \n",
       "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  elephantbird hey dear happy friday to you alre...  \n",
       "1  ughhh layin downnnn waiting for zeina to cook ...  \n",
       "2  greeniebach i reckon hell play even if hes not...  \n",
       "3                  valewee i know saw it on the news  \n",
       "4  very sad that httpwwwfabchannelcom has closed ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12 distinct words.\n",
      "\n",
      "['learn', 'learning', 'is', 'i', 'think', 'so', 'can', 'coding', 'much', 'fun', 'machines', 'machine']\n",
      "[[0 0 1 0]\n",
      " [1 1 0 0]\n",
      " [1 1 0 1]\n",
      " [0 1 0 1]\n",
      " [0 1 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Document term matrix example\n",
    "example_texts = [\n",
    "    'machine learning is so much fun',\n",
    "    'i think learning is fun',\n",
    "    'machines can learn',\n",
    "    'i think coding is fun'\n",
    "]\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = set()\n",
    "for text in example_texts:\n",
    "    words = text.split(' ')\n",
    "    vocab.update(set(words))\n",
    "\n",
    "# fix the set as a list\n",
    "vocab_list = list(vocab)\n",
    "print('Vocabulary Size: {} distinct words.\\n'.format(len(vocab_list)))\n",
    "\n",
    "# initialize empty term-document matrix\n",
    "d_t_matrix = np.zeros((len(vocab), len(example_texts)), dtype=np.intc)\n",
    "for doc_ix_col, text in enumerate(example_texts):\n",
    "    text_words = text.split(' ')\n",
    "    row_ixs = [vocab_list.index(word) for word in text_words if word in vocab_list]\n",
    "    d_t_matrix[row_ixs, doc_ix_col] = 1\n",
    "\n",
    "print(vocab_list)\n",
    "print(d_t_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry: ['im', 'in', 'love', 'with', 'aiden', 'l', 'and', 'im', 'kind', 'of', 'hoping', 'grand', 'win', 'just', 'because', 'they', 'are', 'so', 'adorable']\n"
     ]
    }
   ],
   "source": [
    "# Create a document storage matrix\n",
    "clean_texts = df['clean_tweet']\n",
    "docs = {}\n",
    "labels = []\n",
    "for ix, row in enumerate(clean_texts):\n",
    "    # Store the sentiment\n",
    "    labels = tweet_data[ix][0]\n",
    "    docs[ix] = row.split(' ')\n",
    "\n",
    "# See a sample\n",
    "print('Example entry: {}'.format(docs[np.random.choice(ix)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our tweet-vocabulary has 151670 distinct words.\n"
     ]
    }
   ],
   "source": [
    "# We want to keep track of how many unique words there are:\n",
    "num_nonzero = 0\n",
    "vocab = set()\n",
    "\n",
    "for word_list in docs.values():\n",
    "    unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "    vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "    num_nonzero += len(unique_terms) # add count of unique terms in this tweet\n",
    "\n",
    "doc_key_list = list(docs.keys())\n",
    "\n",
    "print('Our tweet-vocabulary has {} distinct words.'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert everything to a numpy array:\n",
    "doc_key_list = np.array(doc_key_list)\n",
    "vocab = np.array(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['roomyay' 'videobut' 'maristocs' 'baffled' 'beeming']\n",
      "Sorted Vocab: ['a' 'aa' 'aaa' 'aaaa' 'aaaaa']\n"
     ]
    }
   ],
   "source": [
    "# We should keep track of how the vocab/term indices map to the matrix so that we can look them up later.\n",
    "vocab_sorter = np.argsort(vocab)\n",
    "\n",
    "print('Vocab: {}'.format(vocab[:5]))\n",
    "print('Sorted Vocab: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our sparse matrix:\n",
    "num_docs = len(doc_key_list)\n",
    "vocab_size = len(vocab)\n",
    "# A COO matrix is just a tuple of data, row indices, and column indices. Everything else is assumed to be zero.\n",
    "data = np.empty(num_nonzero, dtype=np.intc)     # all non-zero\n",
    "rows = np.empty(num_nonzero, dtype=np.intc)     # row index\n",
    "cols = np.empty(num_nonzero, dtype=np.intc)     # column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full term-document matrix (sparse), please wait!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "# go through all documents with their terms\n",
    "print('Computing full term-document matrix (sparse), please wait!')\n",
    "for doc_key, terms in docs.items():\n",
    "    # find indices to insert-into such that, if the corresponding elements were\n",
    "    # inserted before the indices, the order would be preserved\n",
    "    term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "    # count the unique terms of the document and get their vocabulary indices\n",
    "    uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "    n_vals = len(uniq_indices)  # = number of unique terms\n",
    "    ix_end = ix + n_vals # Add count to index.\n",
    "\n",
    "    data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "    cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "    doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "    rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "\n",
    "    ix = ix_end  # resume with next document -> will add future data on the end.\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five terms alphabetically: ['a' 'aa' 'aaa' 'aaaa' 'aaaaa']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at our sorted vocabulary again\n",
    "print('First five terms alphabetically: {}'.format(vocab[vocab_sorter[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sparse matrix!\n",
    "doc_term_mat = coo_matrix((data, (rows, cols)), shape=(num_docs, vocab_size), dtype=np.intc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab index of math : 149356\n",
      "\n",
      "1st document index containing said word: 171\n",
      "\n",
      "Tweet: [1, 'yayyy, i miss out on science and math tomoro!!  but then i have french in the morning. eugh, fair trade =__=', 'yayyy i miss out on science and math tomoro but then i have french in the morning eugh fair trade']\n",
      "\n",
      "Document-Term Matrix entry: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's check to make sure!\n",
    "vocab_list = list(vocab)\n",
    "word_of_interest = 'math'\n",
    "vocab_interesting_ix = list(vocab).index(word_of_interest)\n",
    "print('vocab index of {} : {}'.format(word_of_interest, vocab_interesting_ix))\n",
    "# Find which tweets contain word:\n",
    "doc_ix_with_word = []\n",
    "for ix, row in enumerate(tweet_data): # Note on this line later.\n",
    "    if word_of_interest in row[1]:\n",
    "        doc_ix_with_word.append(ix)\n",
    "\n",
    "print('\\n1st document index containing said word: {}'.format(doc_ix_with_word[0]))\n",
    "print('\\nTweet: {}'.format(tweet_data[doc_ix_with_word[0]]))\n",
    "\n",
    "# Document - term matrix relevant entry:\n",
    "document_row = doc_ix_with_word[0]\n",
    "vocab_col = vocab_interesting_ix\n",
    "mat_entry = doc_term_mat.tocsr()[document_row, vocab_col]\n",
    "\n",
    "print('\\nDocument-Term Matrix entry: {}'.format(mat_entry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 3 2 1]]\n"
     ]
    }
   ],
   "source": [
    "word_counts = doc_term_mat.sum(axis=0)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words w/counts above 15 : 6228\n"
     ]
    }
   ],
   "source": [
    "# Look at how many words are above a specific cutoff\n",
    "cutoff = 15\n",
    "word_count_list = word_counts.tolist()[0]\n",
    "# Find which column indices are above cutoff\n",
    "col_cutoff_ix = [ix for ix, count in enumerate(word_count_list) if count > cutoff]\n",
    "\n",
    "print('Number of words w/counts above {} : {}'.format(cutoff, len(col_cutoff_ix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix before trimming: (160000, 151670)\n",
      "Shape of document-term matrix after trimming: (160000, 6228)\n"
     ]
    }
   ],
   "source": [
    "# Get the trimmed vocabulary\n",
    "vocab_trimmed = np.array([vocab[x] for x in col_cutoff_ix])\n",
    "# Re-do the vocab-sorter\n",
    "vocab_sorter_trimmed = np.argsort(vocab_trimmed)\n",
    "\n",
    "print('Shape of document-term matrix before trimming: {}'.format(doc_term_mat.shape))\n",
    "\n",
    "# Trim the document-term matrix\n",
    "doc_term_mat_trimmed = doc_term_mat.tocsc()[:,col_cutoff_ix]\n",
    "\n",
    "print('Shape of document-term matrix after trimming: {}'.format(doc_term_mat_trimmed.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'aa', 'aaa', 'aaaah', 'aaah', 'aah', 'aaron', 'ab',\n",
       "       'abandoned', 'abby'], dtype='<U37')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first 10 words alphabetically\n",
    "vocab_trimmed[vocab_sorter_trimmed[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHW5JREFUeJzt3XuUXWWd5vHv0wnhagiXgsYkWlFKJbJUIINBux2GKARUgg6sTsY20Y6TUfG6dDS03aYFcYHTS2xsZaRNhuA4BBptySB0zAqgyzFgimsIAVNyS5FICnPhJkLwN3/sX+GmcqrqTZ0kJ0k9n7XOOnv/9rv3ft+Tk3rOvtQpRQRmZmYl/qzVHTAzsz2HQ8PMzIo5NMzMrJhDw8zMijk0zMysmEPDzMyKOTTsJZJWSTq51f1oJUnvk7RW0tOSjmthP66Q9NVW7d+sPw6NYULSw5Le2af2IUm/6J2PiDdGxC2DbKddUkgauZO62mr/CHwiIg6KiDtb3ZndRd/3yi7c54sZ4L2Pk2vL2yXdLOlZSfc3eH9/VtJvJW2RtEDSvruy/3srh4btVnaDMHo1sGpX7Ww3GG9LSdpf0isGaLI8A7z3cUtt2VXAncBhwJeAayW15XZPA+YCU4B24DXAV3bCEIYdh4a9pH40IulESZ2SnpT0uKRvZLOf5/Pm/OR3kqQ/k/R3kh6RtEHSlZIOrm13Zi77naS/77Off5B0raT/LelJ4EO57+WSNktaL+mfJY2qbS8kfVzSGklPSbpA0mtznSclXVNv32eMDfsqaV9JTwMjgLsl/abBul+R9K2c3kfSM5K+nvP7S3pO0iE5f2ae7tss6RZJx/R5nb8o6R7gGUkjJR0n6Y4cz9XAfoP8W/1XSauz/X2Sjs/6Mbm/zbn/M2vr3CLpI7X5lx095Ov60XxdN0n6tirHAP8TOCn/zTdn+zNy309JekzS5wfqc5/+T5b0XWAdcGzperX1XwccD8yLiN9HxA+BlcB/ziazgPkRsSoiNgEXAB/a3v1YAxHhxzB4AA8D7+xT+xDwi0ZtgOXAB3P6IGByTrcDAYysrfc3QBfVp7mDgB8B389lE4Gngb8ARlGd/nmhtp9/yPmzqD7E7A+cAEwGRub+VgOfqe0vgMXAaOCNwB+AZbn/g4H7gFn9vA799rW27aP7WfcUYGVOvw34DXBbbdndOf064BngXcA+wBdyn6Nqr/NdwPgc7yjgEeCz2f7sfE2+2k8/zgEeA/4DIOBoqiOkfXI/f5vbPAV4Cnh9rncL8JEB/v0DuB4YA7wK6AGmNmqbtfXAX+b0IcDxg7wHj8rXYnW+dvOACQO0/1C+jk8Avwb+nnzfAe8DVvdp/8/At3L6buCvassOz/Ed1ur/i3v6w0caw8uP8xPo5vy0+J0B2r4AHC3p8Ih4OiJuHaDtB4BvRMSDEfE0cB4wPU+9nA3834j4RUQ8D3yZ6j9v3fKI+HFE/DGqT423R8StEbE1Ih4Gvgv8xz7rXBwRT0bEKuBe4Ke5/y3AjUB/F7EH6utglgMdkg4D3gHMB8ZKOij797Ns91fATyJiaUS8QBWU+1MFTa9LI2JtRPyeKiD3Ab4ZES9ExLXAigH68RHg6xGxIipdEfFIbucg4KKIeD4ibqIKgRkFY+t1UURsjohHgZuBtwzQ9gVgoqTREbEpIu5o1EjSqyRdTxXmbwD+G1UwfyUiHhpg+z+nOgo5guoIYgbw33PZQcCWPu23AK/oZ3nv9ECnwqyAQ2N4OSsixvQ+gI8P0HY21Sfm+yWtkPSeAdq+kuqTcq9HqI4Sjsxla3sXRMSzwO/6rL+2PiPpdZKuV3UR80nga1SfFOser03/vsH8QUPo64DyB3wnVUC8gyokfgm8nZeHxsv2ERF/pBrj2Nrm6mN+JfBY5EfiWr/6M57qk3pfrwTW5v7q2xnboG1/flubfpb+X0eofpCfATwi6WeSTuqn3YFUR4TdVEcAq/uMtaEM9ofyw8RK4HyqDyFQHb2O7rPKaKojq0bLe6efwpri0LCGImJNRMyg+pR3MdVFxgPZ9igBqvPSr67NvwrYSvWDfD0wrneBpP2pLly+bHd95i8D7gc6ImI01ekWDX00xX0t8TOq0z7HUR0N/Aw4DTiRP13vedk+JInqB/1jte3Ux7ye6oilPsZXDdCHtcBrG9TXAeMl1f9fv6q232eAA2rL/nyAffS1zb97HulMo3qP/Bi4puGKEaupTgd+iurU4xpJiyWdre27oyn40/tgFfAavfwi+pv5000Mq3K+vuzxiOj7gcW2k0PDGpL015La8lPr5iy/SHWe+49UPwR6XQV8VtKEPFXzNeDqiNgKXAu8V9Lb8uL0Vxg8AF4BPAk8LekNwMd22MAG7muJnwEzgfvydNstVKeLHoqInmxzDfBuSVMk7QN8juq6yy/72eZyquD6VF4Ufz9VCPXne8DnJZ2QF6qPlvRq4DaqYPhCXqg/GXgvsCjXuwt4v6QDJB1NdTRZ6nFgXO8NBpJGSfqApIPzFNyTVO+PhvI02s0RMZMqQK8DPg2sl/SmRutIOl3SkTn9BqprGtfl9n6d45knaT9J7wPeBPwwV78SmC1pYt6c8HfAFdsxXuuHQ8P6MxVYlXcU/RMwPSKey9NLFwL/L6+NTAYWAN+n+qT9EPAc8EmAvObwSaofXOupTg9soPoh2p/PA/8l2/4LcPUOHFe/fS30S6rrE71HFfflNnrniYgHgL8GvkV1Efe9wHszZLaR9fdTXfjdRHVN5Ef9dSAi/pXq3+D/UL1GPwYOze2cCZye+/0OMDMi7s9VLwGepwqAhcAPtmPcN1F9ev+tpCey9kHg4TyF+NEc86Ai4qmImB8Rf0kVjhv6aToFuEfSM8ANVK/J12rLpwOTqF6zi4Cze4M7Iv4d+DrVdZlH8jGvcKw2ABWcWjTbYfLT/WaqU08DXQQ1s92QjzRsp5P03jwlciDVnUQrqW47NbM9jEPDdoVpVBdp1wEdVKe6fIhrtgfy6SkzMyvmIw0zMyu2131Z2uGHHx7t7e2t7oaZ2R7l9ttvfyIi2gZrt9eFRnt7O52dna3uhpnZHkXSQN9C8BKfnjIzs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK+bQMDOzYnvdb4Q3o33uTxrWH77o3bu4J2Zmu6dBjzQkLZC0QdK9DZZ9XlJIOjznJelSSV2S7pF0fK3tLElr8jGrVj9B0spc59Lev5Ms6VBJS7P90vyTjWZm1kIlp6euoPrTny8jaTzwLuDRWvl0qr+X0AHMAS7LtodS/anFt1L9ecd5tRC4LNv2rte7r7nAsojoAJblvJmZtdCgoRERPwc2Nlh0CfAFoP4HOaYBV+Yfkb8VGCPpKOA0YGlEbIyITcBSYGouGx0Ry/OP8lwJnFXb1sKcXlirm5lZiwzpQrikM4HHIuLuPovGAmtr891ZG6je3aAOcGRErAfI5yMG6M8cSZ2SOnt6eoYwIjMzK7HdoSHpAOBLwJcbLW5QiyHUt0tEXB4RkyJiUlvboF8Hb2ZmQzSUI43XAhOAuyU9DIwD7pD051RHCuNrbcdR/V3ogerjGtQBHs/TV+TzhiH01czMdqDtDo2IWBkRR0REe0S0U/3gPz4ifgssBmbmXVSTgS15amkJcKqkQ/IC+KnAklz2lKTJedfUTOC63NVioPcuq1m1upmZtUjJLbdXAcuB10vqljR7gOY3AA8CXcC/AB8HiIiNwAXAinycnzWAjwHfy3V+A9yY9YuAd0laQ3WX1kXbNzQzM9vRBv3lvoiYMcjy9tp0AOf2024BsKBBvRM4tkH9d8CUwfpnZma7jr9GxMzMijk0zMysmEPDzMyKOTTMzKyYQ8PMzIo5NMzMrJhDw8zMijk0zMysmEPDzMyKOTTMzKyYQ8PMzIo5NMzMrJhDw8zMijk0zMysmEPDzMyKOTTMzKyYQ8PMzIo5NMzMrJhDw8zMijk0zMys2KChIWmBpA2S7q3V/oek+yXdI+nfJI2pLTtPUpekBySdVqtPzVqXpLm1+gRJt0laI+lqSaOyvm/Od+Xy9h01aDMzG5qSI40rgKl9akuBYyPiTcCvgfMAJE0EpgNvzHW+I2mEpBHAt4HTgYnAjGwLcDFwSUR0AJuA2VmfDWyKiKOBS7KdmZm10KChERE/Bzb2qf00Irbm7K3AuJyeBiyKiD9ExENAF3BiProi4sGIeB5YBEyTJOAU4NpcfyFwVm1bC3P6WmBKtjczsxbZEdc0/ga4MafHAmtry7qz1l/9MGBzLYB66y/bVi7fku23IWmOpE5JnT09PU0PyMzMGmsqNCR9CdgK/KC31KBZDKE+0La2LUZcHhGTImJSW1vbwJ02M7MhGznUFSXNAt4DTImI3h/m3cD4WrNxwLqcblR/AhgjaWQeTdTb926rW9JI4GD6nCYzM7Nda0hHGpKmAl8EzoyIZ2uLFgPT886nCUAH8CtgBdCRd0qNorpYvjjD5mbg7Fx/FnBdbVuzcvps4KZaOJmZWQsMeqQh6SrgZOBwSd3APKq7pfYFlua16Vsj4qMRsUrSNcB9VKetzo2IF3M7nwCWACOABRGxKnfxRWCRpK8CdwLzsz4f+L6kLqojjOk7YLxmZtaEQUMjImY0KM9vUOttfyFwYYP6DcANDeoPUt1d1bf+HHDOYP0zM7Ndx78RbmZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWbNDQkLRA0gZJ99Zqh0paKmlNPh+SdUm6VFKXpHskHV9bZ1a2XyNpVq1+gqSVuc6lkjTQPszMrHVKjjSuAKb2qc0FlkVEB7As5wFOBzryMQe4DKoAAOYBbwVOBObVQuCybNu73tRB9mFmZi0yaGhExM+BjX3K04CFOb0QOKtWvzIqtwJjJB0FnAYsjYiNEbEJWApMzWWjI2J5RARwZZ9tNdqHmZm1yFCvaRwZEesB8vmIrI8F1tbadWdtoHp3g/pA+9iGpDmSOiV19vT0DHFIZmY2mB19IVwNajGE+naJiMsjYlJETGpra9ve1c3MrNBQQ+PxPLVEPm/IejcwvtZuHLBukPq4BvWB9mFmZi0y1NBYDPTeATULuK5Wn5l3UU0GtuSppSXAqZIOyQvgpwJLctlTkibnXVMz+2yr0T7MzKxFRg7WQNJVwMnA4ZK6qe6Cugi4RtJs4FHgnGx+A3AG0AU8C3wYICI2SroAWJHtzo+I3ovrH6O6Q2t/4MZ8MMA+zMysRQYNjYiY0c+iKQ3aBnBuP9tZACxoUO8Ejm1Q/12jfZiZWev4N8LNzKyYQ8PMzIo5NMzMrJhDw8zMijk0zMysmEPDzMyKOTTMzKyYQ8PMzIo5NMzMrJhDw8zMijk0zMysmEPDzMyKOTTMzKyYQ8PMzIo5NMzMrJhDw8zMijk0zMysmEPDzMyKOTTMzKyYQ8PMzIo1FRqSPitplaR7JV0laT9JEyTdJmmNpKsljcq2++Z8Vy5vr23nvKw/IOm0Wn1q1rokzW2mr2Zm1rwhh4akscCngEkRcSwwApgOXAxcEhEdwCZgdq4yG9gUEUcDl2Q7JE3M9d4ITAW+I2mEpBHAt4HTgYnAjGxrZmYt0uzpqZHA/pJGAgcA64FTgGtz+ULgrJyelvPk8imSlPVFEfGHiHgI6AJOzEdXRDwYEc8Di7KtmZm1yJBDIyIeA/4ReJQqLLYAtwObI2JrNusGxub0WGBtrrs12x9Wr/dZp7/6NiTNkdQpqbOnp2eoQzIzs0E0c3rqEKpP/hOAVwIHUp1K6it6V+ln2fbWty1GXB4RkyJiUltb22BdNzOzIWrm9NQ7gYcioiciXgB+BLwNGJOnqwDGAetyuhsYD5DLDwY21ut91umvbmZmLdJMaDwKTJZ0QF6bmALcB9wMnJ1tZgHX5fTinCeX3xQRkfXpeXfVBKAD+BWwAujIu7FGUV0sX9xEf83MrEkjB2/SWETcJula4A5gK3AncDnwE2CRpK9mbX6uMh/4vqQuqiOM6bmdVZKuoQqcrcC5EfEigKRPAEuo7sxaEBGrhtpfMzNr3pBDAyAi5gHz+pQfpLrzqW/b54Bz+tnOhcCFDeo3ADc000czM9tx/BvhZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFmgoNSWMkXSvpfkmrJZ0k6VBJSyWtyedDsq0kXSqpS9I9ko6vbWdWtl8jaVatfoKklbnOpZLUTH/NzKw5zR5p/BPw7xHxBuDNwGpgLrAsIjqAZTkPcDrQkY85wGUAkg4F5gFvBU4E5vUGTbaZU1tvapP9NTOzJgw5NCSNBt4BzAeIiOcjYjMwDViYzRYCZ+X0NODKqNwKjJF0FHAasDQiNkbEJmApMDWXjY6I5RERwJW1bZmZWQs0c6TxGqAH+F+S7pT0PUkHAkdGxHqAfD4i248F1tbW787aQPXuBvVtSJojqVNSZ09PTxNDMjOzgTQTGiOB44HLIuI44Bn+dCqqkUbXI2II9W2LEZdHxKSImNTW1jZwr83MbMiaCY1uoDsibsv5a6lC5PE8tUQ+b6i1H19bfxywbpD6uAZ1MzNrkSGHRkT8Flgr6fVZmgLcBywGeu+AmgVcl9OLgZl5F9VkYEuevloCnCrpkLwAfiqwJJc9JWly3jU1s7YtMzNrgZFNrv9J4AeSRgEPAh+mCqJrJM0GHgXOybY3AGcAXcCz2ZaI2CjpAmBFtjs/Ijbm9MeAK4D9gRvzYWZmLdJUaETEXcCkBoumNGgbwLn9bGcBsKBBvRM4tpk+mpnZjuPfCDczs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK9Z0aEgaIelOSdfn/ARJt0laI+lqSaOyvm/Od+Xy9to2zsv6A5JOq9WnZq1L0txm+2pmZs3ZEUcanwZW1+YvBi6JiA5gEzA767OBTRFxNHBJtkPSRGA68EZgKvCdDKIRwLeB04GJwIxsa2ZmLdJUaEgaB7wb+F7OCzgFuDabLATOyulpOU8un5LtpwGLIuIPEfEQ0AWcmI+uiHgwIp4HFmVbMzNrkWaPNL4JfAH4Y84fBmyOiK053w2MzemxwFqAXL4l279U77NOf/VtSJojqVNSZ09PT5NDMjOz/gw5NCS9B9gQEbfXyw2axiDLtre+bTHi8oiYFBGT2traBui1mZk1Y2QT674dOFPSGcB+wGiqI48xkkbm0cQ4YF227wbGA92SRgIHAxtr9V71dfqrm5lZCwz5SCMizouIcRHRTnUh+6aI+ABwM3B2NpsFXJfTi3OeXH5TRETWp+fdVROADuBXwAqgI+/GGpX7WDzU/pqZWfOaOdLozxeBRZK+CtwJzM/6fOD7krqojjCmA0TEKknXAPcBW4FzI+JFAEmfAJYAI4AFEbFqJ/TXzMwK7ZDQiIhbgFty+kGqO5/6tnkOOKef9S8ELmxQvwG4YUf00czMmuffCDczs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK+bQMDOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzKzbk0JA0XtLNklZLWiXp01k/VNJSSWvy+ZCsS9Klkrok3SPp+Nq2ZmX7NZJm1eonSFqZ61wqSc0M1szMmtPMkcZW4HMRcQwwGThX0kRgLrAsIjqAZTkPcDrQkY85wGVQhQwwD3grcCIwrzdoss2c2npTm+ivmZk1acihERHrI+KOnH4KWA2MBaYBC7PZQuCsnJ4GXBmVW4Exko4CTgOWRsTGiNgELAWm5rLREbE8IgK4srYtMzNrgR1yTUNSO3AccBtwZESshypYgCOy2VhgbW217qwNVO9uUG+0/zmSOiV19vT0NDscMzPrR9OhIekg4IfAZyLiyYGaNqjFEOrbFiMuj4hJETGpra1tsC6bmdkQNRUakvahCowfRMSPsvx4nloinzdkvRsYX1t9HLBukPq4BnUzM2uRZu6eEjAfWB0R36gtWgz03gE1C7iuVp+Zd1FNBrbk6aslwKmSDskL4KcCS3LZU5Im575m1rZlZmYtMLKJdd8OfBBYKemurP0tcBFwjaTZwKPAObnsBuAMoAt4FvgwQERslHQBsCLbnR8RG3P6Y8AVwP7AjfkwM7MWGXJoRMQvaHzdAWBKg/YBnNvPthYACxrUO4Fjh9pHMzPbsfwb4WZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsUcGmZmVsyhYWZmxRwaZmZWzKFhZmbFHBpmZlbMoWFmZsWa+Rvhw0b73J/0u+zhi969C3tiZtZaPtIwM7Niu31oSJoq6QFJXZLmtro/ZmbD2W59ekrSCODbwLuAbmCFpMURcV9re/Yn/Z268mkrM9sb7dahAZwIdEXEgwCSFgHTgN0mNPoz0HWQnc2BZWY7y+4eGmOBtbX5buCtfRtJmgPMydmnJT0wxP0dDjwxxHV3G7p4u1fZK8Y9BMN13DB8x+5x9+/VJRva3UNDDWqxTSHicuDypncmdUbEpGa3s6fxuIef4Tp2j7t5u/uF8G5gfG1+HLCuRX0xMxv2dvfQWAF0SJogaRQwHVjc4j6ZmQ1bu/XpqYjYKukTwBJgBLAgIlbtxF02fYprD+VxDz/Ddewed5MUsc0lAjMzs4Z299NTZma2G3FomJlZMYdG2pu/rkTSAkkbJN1bqx0qaamkNfl8SNYl6dJ8He6RdHzret4cSeMl3SxptaRVkj6d9b167JL2k/QrSXfnuL+S9QmSbstxX503lyBp35zvyuXtrex/sySNkHSnpOtzfq8ft6SHJa2UdJekzqztlPe5Q4OXfV3J6cBEYIakia3t1Q51BTC1T20usCwiOoBlOQ/Va9CRjznAZbuojzvDVuBzEXEMMBk4N/9d9/ax/wE4JSLeDLwFmCppMnAxcEmOexMwO9vPBjZFxNHAJdluT/ZpYHVtfriM+z9FxFtqv4+xc97nETHsH8BJwJLa/HnAea3u1w4eYztwb23+AeConD4KeCCnvwvMaNRuT38A11F9j9mwGTtwAHAH1TcpPAGMzPpL73mquxNPyumR2U6t7vsQxzsuf0CeAlxP9QvCw2HcDwOH96ntlPe5jzQqjb6uZGyL+rKrHBkR6wHy+Yis75WvRZ56OA64jWEw9jxFcxewAVgK/AbYHBFbs0l9bC+NO5dvAQ7btT3eYb4JfAH4Y84fxvAYdwA/lXR7fq0S7KT3+W79exq7UNHXlQwTe91rIekg4IfAZyLiSanREKumDWp75Ngj4kXgLZLGAP8GHNOoWT7vFeOW9B5gQ0TcLunk3nKDpnvVuNPbI2KdpCOApZLuH6BtU+P2kUZlOH5dyeOSjgLI5w1Z36teC0n7UAXGDyLiR1keFmMHiIjNwC1U13TGSOr9oFgf20vjzuUHAxt3bU93iLcDZ0p6GFhEdYrqm+z94yYi1uXzBqoPCSeyk97nDo3KcPy6ksXArJyeRXW+v7c+M++wmAxs6T3E3dOoOqSYD6yOiG/UFu3VY5fUlkcYSNofeCfVheGbgbOzWd9x974eZwM3RZ7s3pNExHkRMS4i2qn+D98UER9gLx+3pAMlvaJ3GjgVuJed9T5v9QWc3eUBnAH8murc75da3Z8dPLargPXAC1SfMmZTnbtdBqzJ50OzrajuJPsNsBKY1Or+NzHuv6A67L4HuCsfZ+ztYwfeBNyZ474X+HLWXwP8CugC/hXYN+v75XxXLn9Nq8ewA16Dk4Hrh8O4c3x352NV78+vnfU+99eImJlZMZ+eMjOzYg4NMzMr5tAwM7NiDg0zMyvm0DAzs2IODTMzK+bQMDOzYv8f94mgoohdQdYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count of words for each appearance\n",
    "hist_breaks = np.arange(0, 500, 10)\n",
    "plt.hist(word_count_list, bins = hist_breaks)\n",
    "plt.title('Histogram of word counts < 500')\n",
    "plt.show()\n",
    "\n",
    "# Too many words appear few times. Check out trimmed.\n",
    "trimmed_word_counts = doc_term_mat_trimmed.sum(axis=0)\n",
    "trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "hist_breaks = np.arange(0, 500, 5)\n",
    "plt.hist(trimmed_word_list, bins = hist_breaks)\n",
    "plt.title('Histogram of trimmed word counts < 500')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the TFIDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, max_features=6228, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer over the dataset\n",
    "clean_texts = df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For training 70% data will be 120,000 and remaining 30 % or 40,000 will be testing data\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into train-test. Please wait!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Generate 40,000 random row indices\n",
    "print('Splitting into train-test. Please wait!')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets,\n",
    "                                                    y_targets,\n",
    "                                                    test_size=40000,\n",
    "                                                    random_state=42)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Build a classifier that classifies the sentiment of a sentence.\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a standard Logistic Model training!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Starting a standard Logistic Model training!')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at model object\n",
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: What is the accuracy of your model when applied to testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: What is the accuracy of your model when applied to testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7801666666666667\n",
      "Test accuracy: 0.756375\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: What conclusions can you draw from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14596  5460]\n",
      " [ 4285 15659]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.77305227 0.74146503]\n",
      "Recall   : [0.72776227 0.78514842]\n",
      "F1       : [0.74972391 0.76268173]\n",
      "Support  : [20056 19944]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# Remember:\n",
    "# Precision is the proportion of correct predictions among all predicted\n",
    "# Recall (sensitivity) is the proportion of correct predictions among all true actual examples\n",
    "# F1 is the harmonic average of precision and recall\n",
    "# Support is count of actual cases of specific class\n",
    "# Here, each of the following is a pair of numbers, the first is for class 1 ('1') and second for class 0 ('0')\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "\n",
    "# Get the parts of the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "# Print results\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training regularized logistic regression\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Starting training regularized logistic regression')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "lr_reg = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.0001, l1_ratio=0.15)\n",
    "lr_reg.fit(X_train, y_train)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=None,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='elasticnet',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at model object\n",
    "lr_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Is it better to have a model per source?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr_reg.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr_reg.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7526083333333333\n",
      "Test accuracy: 0.745825\n"
     ]
    }
   ],
   "source": [
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notice how our train and test accuracies are much closer together! Although we lost ~1 percentage point in the accuracy, we might consider accepting this because it is less over fit to our training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
