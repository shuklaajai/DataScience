{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ajai Shukla\n",
    "Create a new notebook and perform each of the following tasks and answer the related questions:\n",
    "\n",
    "    1). Build a simple neural networks model\n",
    "    2). Build a DNN model\n",
    "    3). Build a RNN model\n",
    "    Summarize your findings with examples.  Explain what the manufacturer should focus on to optimize the diaper manufacturing process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ashukla\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ashukla\\anaconda3\\lib\\site-packages (from pandas) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ashukla\\anaconda3\\lib\\site-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ashukla\\anaconda3\\lib\\site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashukla\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from numpy import loadtxt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Masking\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-alpha0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find(name, path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(url, filename ):\n",
    "    \n",
    "    \n",
    "    ## Read the .csv file with the pandas read_csv method\n",
    "    data_url = pd.read_csv(url, sep = \" \", header=None)\n",
    "    data_url.to_csv(filename,sep=\",\", index=False)\n",
    "    file_path = x=os.getcwd()\n",
    "    #print file_path\n",
    "    file_name = find(filename, file_path)\n",
    "    #print file_name\n",
    "    data = pd.read_csv(file_name)\n",
    "    nRow, nCol = data.shape\n",
    "    print(f'There are {nRow} rows and {nCol} columns')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1567 rows and 590 columns\n"
     ]
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "filename = 'secom.csv'\n",
    "secom_data = data(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1567 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "filename = 'labels.csv'\n",
    "labels = data(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.rename(columns={\"0\": \"classification\", \"1\": \"timestamp\"})\n",
    "dataset = pd.concat([secom_data, labels['classification']], axis=1)\n",
    "#secom = dataset[['54','59', '162', '348', '172', '427', '134', '420', '320', 'classification']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secom['classification'] = secom['classification'].replace([-1], 0)\n",
    "dataset['classification'] = dataset['classification'].replace([-1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1          2          3       4      5         6       7  \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
       "3  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
       "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
       "\n",
       "        8       9  ...       581     582     583     584      585     586  \\\n",
       "0  1.5005  0.0162  ...       NaN  0.5005  0.0118  0.0035   2.3630     NaN   \n",
       "1  1.4966 -0.0005  ...  208.2045  0.5019  0.0223  0.0055   4.4447  0.0096   \n",
       "2  1.4436  0.0041  ...   82.8602  0.4958  0.0157  0.0039   3.1745  0.0584   \n",
       "3  1.4882 -0.0124  ...   73.8432  0.4990  0.0103  0.0025   2.0544  0.0202   \n",
       "4  1.5031 -0.0031  ...       NaN  0.4800  0.4766  0.1045  99.3032  0.0202   \n",
       "\n",
       "      587     588       589  classification  \n",
       "0     NaN     NaN       NaN               0  \n",
       "1  0.0201  0.0060  208.2045               0  \n",
       "2  0.0484  0.0148   82.8602               1  \n",
       "3  0.0149  0.0044   73.8432               0  \n",
       "4  0.0149  0.0044   73.8432               0  \n",
       "\n",
       "[5 rows x 591 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().any().any()\n",
    "# Replacing all the NaN values with 0 as the values correspond to the test results.\n",
    "dataset = dataset.replace(np.NaN, 0)\n",
    "# again, checking if there is any NULL values left\n",
    "dataset.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "x = dataset.drop(columns=['classification'], axis=1)\n",
    "y = dataset['classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our input features are at different scales we need to standardize the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13999828,  0.42920791,  0.03273532, ..., -1.86846064,\n",
       "        -1.84076867, -1.0611589 ],\n",
       "       [ 0.46402024, -0.10587396,  0.23685169, ...,  0.41258698,\n",
       "         0.25102908,  1.15695081],\n",
       "       [-0.35125598,  0.40723307,  0.02641324, ...,  3.62421124,\n",
       "         3.31899911, -0.17840653],\n",
       "       ...,\n",
       "       [-0.12041844, -0.5678868 ,  0.12151173, ..., -0.89249002,\n",
       "        -0.96918628, -0.59748491],\n",
       "       [-0.53957345,  0.25606135, -0.01842658, ...,  0.91192079,\n",
       "         0.77397852, -0.06511812],\n",
       "       [-0.28974927, -0.18370602,  0.06960583, ..., -0.03000435,\n",
       "        -0.27192036,  0.40672924]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1463, 1: 104})"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the input features and target variables into training dataset and test dataset. out test dataset will be 30% of our entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:  (1096, 590)\n",
      "shape of x_test:  (471, 590)\n",
      "shape of y_train:  (1096,)\n",
      "shape of y_test:  (471,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "# gettiing the shapes\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1096, 590), (1096,))"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape\n",
    "\n",
    "x_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 590)\n",
      "(1096, 590)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train.shape[0:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have preprocessed the data and we are now ready to build the neural network.\n",
    "We are using keras to build our neural network. We import the keras library to create the neural network layers.\n",
    "There are two main types of models available in keras — Sequential and Model. we will use Sequential model to build our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1). Build a simple neural networks model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "#First Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=590))\n",
    "#Second  Hidden Layer\n",
    "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
    "#Output Layer\n",
    "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the neural network\n",
    "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1096/1096 [==============================] - 0s 254us/sample - loss: 0.6895 - accuracy: 0.8951\n",
      "Epoch 2/20\n",
      "1096/1096 [==============================] - 0s 25us/sample - loss: 0.6793 - accuracy: 0.9288\n",
      "Epoch 3/20\n",
      "1096/1096 [==============================] - 0s 22us/sample - loss: 0.6644 - accuracy: 0.9288\n",
      "Epoch 4/20\n",
      "1096/1096 [==============================] - 0s 25us/sample - loss: 0.6388 - accuracy: 0.9288\n",
      "Epoch 5/20\n",
      "1096/1096 [==============================] - 0s 22us/sample - loss: 0.5934 - accuracy: 0.9288\n",
      "Epoch 6/20\n",
      "1096/1096 [==============================] - 0s 24us/sample - loss: 0.5197 - accuracy: 0.9288\n",
      "Epoch 7/20\n",
      "1096/1096 [==============================] - 0s 28us/sample - loss: 0.4320 - accuracy: 0.9288\n",
      "Epoch 8/20\n",
      "1096/1096 [==============================] - 0s 23us/sample - loss: 0.3521 - accuracy: 0.9288\n",
      "Epoch 9/20\n",
      "1096/1096 [==============================] - 0s 28us/sample - loss: 0.2931 - accuracy: 0.9288\n",
      "Epoch 10/20\n",
      "1096/1096 [==============================] - ETA: 0s - loss: 0.2530 - accuracy: 0.93 - 0s 25us/sample - loss: 0.2558 - accuracy: 0.9288\n",
      "Epoch 11/20\n",
      "1096/1096 [==============================] - 0s 29us/sample - loss: 0.2325 - accuracy: 0.9288\n",
      "Epoch 12/20\n",
      "1096/1096 [==============================] - 0s 22us/sample - loss: 0.2169 - accuracy: 0.9288\n",
      "Epoch 13/20\n",
      "1096/1096 [==============================] - 0s 22us/sample - loss: 0.2056 - accuracy: 0.9288\n",
      "Epoch 14/20\n",
      "1096/1096 [==============================] - 0s 28us/sample - loss: 0.1971 - accuracy: 0.9288\n",
      "Epoch 15/20\n",
      "1096/1096 [==============================] - 0s 22us/sample - loss: 0.1903 - accuracy: 0.9288\n",
      "Epoch 16/20\n",
      "1096/1096 [==============================] - 0s 25us/sample - loss: 0.1844 - accuracy: 0.9288\n",
      "Epoch 17/20\n",
      "1096/1096 [==============================] - 0s 28us/sample - loss: 0.1790 - accuracy: 0.9288\n",
      "Epoch 18/20\n",
      "1096/1096 [==============================] - 0s 25us/sample - loss: 0.1738 - accuracy: 0.9288\n",
      "Epoch 19/20\n",
      "1096/1096 [==============================] - 0s 22us/sample - loss: 0.1692 - accuracy: 0.9288\n",
      "Epoch 20/20\n",
      "1096/1096 [==============================] - 0s 27us/sample - loss: 0.1649 - accuracy: 0.9288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x244513229b0>"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the data to the training dataset\n",
    "classifier.fit(x_train,y_train, batch_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the loss value & metrics values for the model in test mode using evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096/1096 [==============================] - 0s 59us/sample - loss: 0.1610 - accuracy: 0.9288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16096634760390233, 0.9288321]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model=classifier.evaluate(x_train, y_train)\n",
    "eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now predict the output for our test dataset. If the prediction is greater than 0.5 then the output is 1 else the output is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=classifier.predict(x_test)\n",
    "y_pred =(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the moment of truth. we check the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[445   0]\n",
      " [ 26   0]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2). Build a DNN model\n",
    "we will now build our model which requires us to configure its layers, and then compile it. For this problem we will have 3 layers: an input layer, a hidden layer, and output layer. We will build a dense network, where all the neurons are connected to the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add model layers\n",
    "model.add(keras.layers.Flatten(input_shape =(n_cols,)))\n",
    "model.add(keras.layers.Dense(512, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(keras.layers.Dense(512, activation='relu'))\n",
    "model.add(keras.layers.Dense(512, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.layers.convolutional.Conv3D"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Conv3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model using mse as a measure of model performance\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 876 samples, validate on 220 samples\n",
      "Epoch 1/30\n",
      "876/876 [==============================] - 0s 385us/sample - loss: 0.4459 - accuracy: 0.8265 - val_loss: 0.0926 - val_accuracy: 0.8955\n",
      "Epoch 2/30\n",
      "876/876 [==============================] - 0s 161us/sample - loss: 0.1597 - accuracy: 0.9224 - val_loss: 0.0923 - val_accuracy: 0.9227\n",
      "Epoch 3/30\n",
      "876/876 [==============================] - 0s 148us/sample - loss: 0.0770 - accuracy: 0.9304 - val_loss: 0.0992 - val_accuracy: 0.9227\n",
      "Epoch 4/30\n",
      "876/876 [==============================] - 0s 149us/sample - loss: 0.0587 - accuracy: 0.9452 - val_loss: 0.1142 - val_accuracy: 0.9182\n",
      "Epoch 5/30\n",
      "876/876 [==============================] - 0s 145us/sample - loss: 0.1272 - accuracy: 0.9555 - val_loss: 0.0779 - val_accuracy: 0.9227\n",
      "Epoch 6/30\n",
      "876/876 [==============================] - 0s 148us/sample - loss: 0.0812 - accuracy: 0.9532 - val_loss: 0.1062 - val_accuracy: 0.9227\n",
      "Epoch 7/30\n",
      "876/876 [==============================] - 0s 155us/sample - loss: 0.1646 - accuracy: 0.9760 - val_loss: 0.0770 - val_accuracy: 0.9227\n",
      "Epoch 8/30\n",
      "876/876 [==============================] - 0s 143us/sample - loss: 0.2772 - accuracy: 0.9795 - val_loss: 0.1179 - val_accuracy: 0.9227\n",
      "Epoch 9/30\n",
      "876/876 [==============================] - 0s 159us/sample - loss: 0.0423 - accuracy: 0.9852 - val_loss: 0.1350 - val_accuracy: 0.9227\n",
      "Epoch 10/30\n",
      "876/876 [==============================] - 0s 148us/sample - loss: 0.2692 - accuracy: 0.9920 - val_loss: 0.0822 - val_accuracy: 0.9182\n",
      "Epoch 11/30\n",
      "876/876 [==============================] - 0s 148us/sample - loss: 0.0222 - accuracy: 0.9909 - val_loss: 0.0739 - val_accuracy: 0.9227\n",
      "Epoch 12/30\n",
      "876/876 [==============================] - 0s 159us/sample - loss: 0.0286 - accuracy: 0.9989 - val_loss: 0.0729 - val_accuracy: 0.9227\n",
      "Epoch 13/30\n",
      "876/876 [==============================] - 0s 156us/sample - loss: 0.0181 - accuracy: 0.9977 - val_loss: 0.0762 - val_accuracy: 0.9227\n",
      "Epoch 14/30\n",
      "876/876 [==============================] - 0s 172us/sample - loss: 0.0171 - accuracy: 0.9977 - val_loss: 0.0732 - val_accuracy: 0.9227\n",
      "Epoch 15/30\n",
      "876/876 [==============================] - 0s 182us/sample - loss: 0.0185 - accuracy: 0.9966 - val_loss: 0.0743 - val_accuracy: 0.9227\n",
      "Epoch 16/30\n",
      "876/876 [==============================] - 0s 166us/sample - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0755 - val_accuracy: 0.9227\n",
      "Epoch 17/30\n",
      "876/876 [==============================] - 0s 182us/sample - loss: 0.0016 - accuracy: 0.9989 - val_loss: 0.0739 - val_accuracy: 0.9227\n",
      "Epoch 18/30\n",
      "876/876 [==============================] - 0s 195us/sample - loss: 8.5004e-04 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9227\n",
      "Epoch 19/30\n",
      "876/876 [==============================] - 0s 162us/sample - loss: 9.0889e-04 - accuracy: 0.9977 - val_loss: 0.0740 - val_accuracy: 0.9227\n",
      "Epoch 20/30\n",
      "876/876 [==============================] - 0s 173us/sample - loss: 3.4201e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9227\n",
      "Epoch 21/30\n",
      "876/876 [==============================] - 0s 183us/sample - loss: 2.7374e-04 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9227\n",
      "Epoch 22/30\n",
      "876/876 [==============================] - 0s 164us/sample - loss: 2.4872e-04 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9227\n",
      "Epoch 23/30\n",
      "876/876 [==============================] - 0s 156us/sample - loss: 2.3290e-04 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9227\n",
      "Epoch 24/30\n",
      "876/876 [==============================] - 0s 147us/sample - loss: 2.6083e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9227\n",
      "Epoch 25/30\n",
      "876/876 [==============================] - 0s 157us/sample - loss: 2.6782e-04 - accuracy: 1.0000 - val_loss: 0.0746 - val_accuracy: 0.9227\n",
      "Epoch 26/30\n",
      "876/876 [==============================] - 0s 147us/sample - loss: 3.9800e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9227\n",
      "Epoch 27/30\n",
      "876/876 [==============================] - 0s 151us/sample - loss: 2.8894e-04 - accuracy: 1.0000 - val_loss: 0.0745 - val_accuracy: 0.9227\n",
      "Epoch 28/30\n",
      "876/876 [==============================] - 0s 184us/sample - loss: 2.2885e-04 - accuracy: 1.0000 - val_loss: 0.0739 - val_accuracy: 0.9227\n",
      "Epoch 29/30\n",
      "876/876 [==============================] - 0s 189us/sample - loss: 2.0832e-04 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9227\n",
      "Epoch 30/30\n",
      "876/876 [==============================] - 0s 165us/sample - loss: 1.3986e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24441ffaf98>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train model\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096/1096 [==============================] - 0s 46us/sample - loss: 0.0149 - accuracy: 0.9845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.014888007728374236, 0.984489]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model=model.evaluate(x_train, y_train)\n",
    "eval_model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We now predict the output for our test dataset. If the prediction is greater than 0.5 then the output is 1 else the output is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_pred =(y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the moment of truth. we check the accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[444   1]\n",
      " [ 25   1]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3). Build a RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the output with the forward step:\n",
    "The forward step will unroll the network, and compute the forward activations just as in regular backpropagation. The final output will be used to compute the loss function from which the error signal used for training will be derived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 590)\n",
      "(590,)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], 1,x_train.shape[1]))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "input_dim = 590\n",
    "\n",
    "units = 128\n",
    "output_size = 128  # labels are from 0 to 9\n",
    "\n",
    "# Build the RNN model\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "  # CuDNN is only available at the layer level, and not at the cell level.\n",
    "  # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "  # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "  if allow_cudnn_kernel:\n",
    "    # The LSTM layer with default options uses CuDNN.\n",
    "    lstm_layer = tf.keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
    "  else:\n",
    "    # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "    lstm_layer = tf.keras.layers.RNN(\n",
    "        tf.keras.layers.LSTMCell(units),\n",
    "        input_shape=(None, input_dim))\n",
    "  model = tf.keras.models.Sequential([\n",
    "      lstm_layer,\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.Dense(output_size, activation='softmax')]\n",
    "  )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(allow_cudnn_kernel=True)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "unified_lstm_126 (UnifiedLST (None, 128)               368128    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_12 (B (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 128)               16512     \n",
      "=================================================================\n",
      "Total params: 385,152\n",
      "Trainable params: 384,896\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1096 samples, validate on 471 samples\n",
      "Epoch 1/5\n",
      "1096/1096 [==============================] - 1s 558us/sample - loss: 4.8379 - accuracy: 0.0611 - val_loss: 4.8177 - val_accuracy: 0.9448\n",
      "Epoch 2/5\n",
      "1096/1096 [==============================] - 0s 80us/sample - loss: 4.8041 - accuracy: 0.3896 - val_loss: 4.7836 - val_accuracy: 0.9448\n",
      "Epoch 3/5\n",
      "1096/1096 [==============================] - 0s 86us/sample - loss: 4.7705 - accuracy: 0.7400 - val_loss: 4.7495 - val_accuracy: 0.9448\n",
      "Epoch 4/5\n",
      "1096/1096 [==============================] - 0s 84us/sample - loss: 4.7369 - accuracy: 0.8604 - val_loss: 4.7151 - val_accuracy: 0.9448\n",
      "Epoch 5/5\n",
      "1096/1096 [==============================] - 0s 80us/sample - loss: 4.7030 - accuracy: 0.8987 - val_loss: 4.6809 - val_accuracy: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x244790e1f98>"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=batch_size,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1096/1096 [==============================] - 0s 46us/sample - loss: 4.6835 - accuracy: 0.9288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.683455905775084, 0.9288321]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model=model.evaluate(x_train, y_train)\n",
    "eval_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize your findings with examples.  Explain what the manufacturer should focus on to optimize the diaper manufacturing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we used Simple neuro network model, DNN model and RNN model to analye SECOM dataset, as per SNN model loss was around 15 % and accuracy was 93 % . and with DNN model loss was 1.6 % and accuracy was 98 % that was showing overfiting.\n",
    "RNN model was not bad and this improved overfitting over other model loss was 4 % and accuracy was 92 %"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
