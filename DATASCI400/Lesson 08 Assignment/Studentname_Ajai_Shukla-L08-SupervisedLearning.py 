#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb 26 00:48:02 2019

@author: ashukla
"""

###################################################################################################################
# Import statements for necessary package(s).
###################################################################################################################
import numpy as np
import pandas as pd
from sklearn import model_selection
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import confusion_matrix
from copy import deepcopy
# import package
#################################################################################
#3.Read in the dataset from a freely and easily available source on the internet.
#################################################################################
url = "http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data"
Mamm = pd.read_csv(url, header=None) 
Mamm.columns = ["BI-RADS", "Age", "Shape", "Margin", "Density", "Severity"]
#################
#4.Show data preparation. 
# Check the data types
Mamm.dtypes
#################
# Check the first rows of the data frame
Mamm.head()
#################
# Check the unique values
Mamm.loc[:, "Shape"].unique()
##################
#Normalize some numeric columns,
#one-hot encode some categorical columns with 3 or more categories,
#remove or replace missing values, remove or replace some outliers.
##################################################################################################
#Normalize some numeric columns
x = np.array([1,11,5,3,15,3,5,9,7,9,3,5,7,3,5,21])
print(" The original variable:", x)
############
offset = 0
spread = 1
xNormTrivial = (x - offset)/spread
print(" Trivial normalization doesn't change values: ", xNormTrivial)
############
offset = min(x)
spread = max(x) - min(x)
xNormMinMax = (x - offset)/spread
print(" The min-max-normalized variable:", xNormMinMax)
offset = np.mean(x)
spread = np.std(x)
xNormZ = (x - offset)/spread
print(" The Z-normalized variable:", xNormZ)

# Compare the values before after the two normalizations
print ("\nScaled variable x using numpy calculations\n")
print(np.hstack(
        (np.reshape(x,(16,1)),
         np.reshape(xNormMinMax,(16,1)),
         np.reshape(xNormZ, (16,1))
        ))
    )
######################################################################################################
#one-hot encode some categorical columns with 3 or more categories,
######################################################################################################

enc = OneHotEncoder(handle_unknown='ignore')
X = [['Male', 1], ['Female', 3], ['Female', 2]]
enc.fit(X)
OneHotEncoder(categorical_features=None, categories=None,
       n_values=None, sparse=True)
print(OneHotEncoder)
######################################################################################################
#remove or replace missing values, remove or replace some outliers
############################################################################################
# Decode Shape
Replace = Mamm.loc[:, "Shape"] == "1"
Mamm.loc[Replace, "Shape"] = "round"

Replace = Mamm.loc[:, "Shape"] == "2"
Mamm.loc[Replace, "Shape"] = "oval"

Replace = Mamm.loc[:, "Shape"] == "3"
Mamm.loc[Replace, "Shape"] = "lobular"

Replace = Mamm.loc[:, "Shape"] == "4"
Mamm.loc[Replace, "Shape"] = "irregular"
###################

# Get the counts for each value
Mamm.loc[:,"Shape"].value_counts()
###################

# Specify all the locations that have a missing value
MissingValue = Mamm.loc[:, "Shape"] == "?"

# Impute missing values
Mamm.loc[MissingValue, "Shape"] = "irregular"
###################

# Decode Margin
Replace = Mamm.loc[:, "Margin"] == "1"
Mamm.loc[Replace, "Margin"] = "circumscribed"

Replace = Mamm.loc[:, "Margin"] == "2"
Mamm.loc[Replace, "Margin"] = "microlobulated"

Replace = Mamm.loc[:, "Margin"] == "3"
Mamm.loc[Replace, "Margin"] = "obscured"

Replace = Mamm.loc[:, "Margin"] == "4"
Mamm.loc[Replace, "Margin"] = "ill-defined"

Replace = Mamm.loc[:, "Margin"] == "5"
Mamm.loc[Replace, "Margin"] = "spiculated"
###################

# Get the counts for each value
Mamm.loc[:,"Margin"].value_counts()

# Specify all the locations that have a missing value
MissingValue = Mamm.loc[:, "Margin"] == "?"

# Impute missing values
Mamm.loc[MissingValue, "Margin"] = "circumscribed"
###################

# Get the counts for each value
Mamm.loc[:,"Shape"].value_counts()

# Get the counts for each value
Mamm.loc[:,"Margin"].value_counts()

####################
#Apply K-Means on some of your columns,  Add the K-Means cluster labels to your dataset.
###################################################################
    
def kmeans(X, k, th):
    if k < 2:
        print('k needs to be at least 2!')
        return
    if (th <= 0.0) or (th >= 1.0):
        print('th values are beyond meaningful bounds')
        return    
    
    N, m = X.shape # dimensions of the dataset
    Y = np.zeros(N, dtype=int) # cluster labels
    C = np.random.uniform(0, 1, [k,m]) # centroids
    d = th + 1.0
    dist_to_centroid = np.zeros(k) # centroid distances
    
    while d > th:
        C_ = deepcopy(C)
        
        for i in range(N): # assign cluster labels to all data points            
            for j in range(k): 
                dist_to_centroid[j] = np.sqrt(sum((X[i,] - C[j,])**2))                
            Y[i] = np.argmin(dist_to_centroid) # assign to most similar cluster            
            
        for j in range(k): # recalculate all the centroids
            ind = FindAll(Y, j) # indexes of data points in cluster j
            n = len(ind)            
            if n > 0: C[j] = sum(X[ind,]) / n
        
        d = np.mean(abs(C - C_)) # how much have the centroids shifted on average?
        
    return Y, C
################################################################################################
#Split your data set into training and testing sets using the proper function in sklearn (include decision comments).
###############################################################################################################
    ####################
""" Auxiliary functions """
def normalize(X): # max-min normalizing
    N, m = X.shape
    Y = np.zeros([N, m])
    
    for i in range(m):
        mX = min(X[:,i])
        Y[:,i] = (X[:,i] - mX) / (max(X[:,i]) - mX)
    
    return Y

####################
def split_dataset(data, r): # split a dataset in matrix format, using a given ratio for the testing set
	N = len(data)	
	X = []
	Y = []
	
	if r >= 1: 
		print ("Parameter r needs to be smaller than 1!")
		return
	elif r <= 0:
		print ("Parameter r needs to be larger than 0!")
		return

	n = int(round(N*r)) # number of elements in testing sample
	nt = N - n # number of elements in training sample
	ind = -np.ones(n,int) # indexes for testing sample
	R = np.random.randint(N) # some random index from the whole dataset
	
	for i in range(n):
		while R in ind: R = np.random.randint(N) # ensure that the random index hasn't been used before
		ind[i] = R

	ind_ = list(set(range(N)).difference(ind)) # remaining indexes	
	X = data[ind_,:-1] # training features
	XX = data[ind,:-1] # testing features
	Y = data[ind_,-1] # training targets
	YY = data[ind,-1] # testing targests
	return X, XX, Y, YY
################################################################################################################
#Create a classification model for the expert label based on the training data (include decision comments).
 #########################################################################################################
r = 0.2 # ratio of test data over all data (this can be changed to any number between 0.0 and 1.0 (not inclusive)
dataset = np.genfromtxt ('https://library.startlearninglabs.uw.edu/DATASCI400/Datasets/iris.csv', delimiter=",")
all_inputs = normalize(dataset[:,:4]) # inputs (features)
normalized_data = deepcopy(dataset)
normalized_data[:,:4] = all_inputs
X, XX, Y, YY = split_dataset(normalized_data, r)
print ('\n\n\nLogistic regression classifier\n')
C_parameter = 50. / len(X) # parameter for regularization of the model
class_parameter = 'ovr' # parameter for dealing with multiple classes
penalty_parameter = 'l1' # parameter for the optimizer (solver) in the function
solver_parameter = 'saga' # optimization system used
tolerance_parameter = 0.1 # termination parameter

#Training the Model
clf = LogisticRegression(C=C_parameter, multi_class=class_parameter, penalty=penalty_parameter, solver=solver_parameter, tol=tolerance_parameter)
clf.fit(X, Y) 
print ('coefficients:')
print (clf.coef_) # each row of this matrix corresponds to each one of the classes of the dataset
print ('intercept:')
print (clf.intercept_) # each element of this vector corresponds to each one of the classes of the dataset

# Apply the Model
print ('predictions for test set:')
print (clf.predict(XX))
print ('actual class values:')
print (YY)   
################################################
#Apply your (trained) classifiers to the test data to predict probabilities.
#####################################################
#####################

# Naive Bayes classifier
print ('\n\nNaive Bayes classifier\n')
nbc = GaussianNB() # default parameters are fine
nbc.fit(X, Y)
print ("predictions for test set:")
print (nbc.predict(XX))
print ('actual class values:')
print (YY)
####################

# k Nearest Neighbors classifier
print ('\n\nK nearest neighbors classifier\n')
k = 5 # number of neighbors
distance_metric = 'euclidean'
knn = KNeighborsClassifier(n_neighbors=k, metric=distance_metric)
knn.fit(X, Y)
print ("predictions for test set:")
print (knn.predict(XX))
print ('actual class values:')
print (YY)
###################

# Support vector machine classifier
t = 0.001 # tolerance parameter
kp = 'rbf' # kernel parameter
print ('\n\nSupport Vector Machine classifier\n')
clf = SVC(kernel=kp, tol=t)
clf.fit(X, Y)
print ("predictions for test set:")
print (clf.predict(XX))
print ('actual class values:')
print (YY)
####################

# Decision Tree classifier
print ('\n\nDecision Tree classifier\n')
clf = DecisionTreeClassifier() # default parameters are fine
clf.fit(X, Y)
print ("predictions for test set:")
print (clf.predict(XX))
print ('actual class values:')
print (YY)
####################

# Random Forest classifier
estimators = 10 # number of trees parameter
mss = 2 # mininum samples split parameter
print ('\n\nRandom Forest classifier\n')
clf = RandomForestClassifier(n_estimators=estimators, min_samples_split=mss) # default parameters are fine
clf.fit(X, Y)
print ("predictions for test set:")
print (clf.predict(XX))
print ('actual class values:')
print (YY)
##############################################################
#Write out to a csv a dataframe of the test data, including actual outcomes, and the probabilities of your classification
################################################################
r = 0.1 # ratio of test data over all data (this can be changed to any number between 0.0 and 1.0 (not inclusive)
dataset = np.genfromtxt ('https://library.startlearninglabs.uw.edu/DATASCI400/Datasets/OnlineNewsPopularity.csv', delimiter=",")
dataset = dataset[1:,39:] # get rid of headers and first 38 columns
all_inputs = normalize(dataset[:,:-1]) # inputs (features)
normalized_data = deepcopy(dataset)
normalized_data[:,:-1] = all_inputs
X, XX, Y, YY = split_dataset(normalized_data, r)
#####################################################################################
#Determine accuracy rate, which is the number of correct predictions divided by the total number of predictions (include brief preliminary analysis commentary).
################################################################################################
#read the file
df = pd.read_csv('https://library.startlearninglabs.uw.edu/DATASCI400/Datasets/iris.csv')
#dataframe = pandas.read_csv(url, names=names)
array = df.values
X = array[:,0:4]
Y = array[:,4]
seed = 4
kfold = model_selection.KFold(n_splits=10, random_state=seed)
model = LogisticRegression()
scoring = 'accuracy'
results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
Accu_res=print("Accuracy: %.3f (%.3f)") 
Acc_res1=(results.mean(), results.std())
final_results=print(Accu_res/Acc_res1)